{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Designed by Tayven Stover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import json\n",
    "\n",
    "TRAINING_DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_useful_columns(df, columns_to_keep):\n",
    "    columns = [col for col in df.columns if col not in columns_to_keep]\n",
    "\n",
    "    df = df.drop(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_mean_location(df):\n",
    "    mean_location = df[['latitude', 'longitude']].mean()\n",
    "\n",
    "    # Convert to list\n",
    "    mean_location = mean_location.to_list()\n",
    "\n",
    "    return mean_location\n",
    "\n",
    "def run_length_encoding(df, column_name):\n",
    "    # Apply RLE: Get run lengths and values\n",
    "    n = len(df)\n",
    "    y = np.array(df[f'{column_name}_is_nan'])\n",
    "    starts = np.r_[0, np.flatnonzero(y[1:] != y[:-1]) + 1]\n",
    "    lengths = np.diff(np.r_[starts, n])\n",
    "    values = y[starts]\n",
    "    return starts, lengths, values\n",
    "\n",
    "def mark_large_gaps(df, column_name, gap_threshold=5):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "     # Flag rows that are NaN\n",
    "    df[f'{column_name}_is_nan'] = df[column_name].isna().astype(int)\n",
    "    \n",
    "    # Apply RLE on the column\n",
    "    starts, lengths, values = run_length_encoding(df, column_name)\n",
    "    \n",
    "    # Initialize the validity column with ones\n",
    "    df[f'{column_name}_valid_sequence'] = 1\n",
    "    \n",
    "    # Identify the start positions of large NaN gaps and their lengths\n",
    "    large_gaps = (values == 1) & (lengths > gap_threshold)\n",
    "\n",
    "    # Set the validity of sequences following large gaps to 0\n",
    "    for start, length in zip(starts[large_gaps], lengths[large_gaps]):\n",
    "        df.loc[start:start+length-1, f'{column_name}_valid_sequence'] = 0\n",
    "    \n",
    "    df.drop([f'{column_name}_is_nan'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def interpolate_missing_values(df, columns_to_interpolate):\n",
    "    # Specify validation columns\n",
    "    validation_columns = [f'{column}_valid_sequence' for column in columns_to_interpolate]\n",
    "    \n",
    "    # Determine rows eligible for interpolation\n",
    "    # Only interpolate rows where all associated validation columns are 1\n",
    "    df['interpolate_flag'] = df[validation_columns].all(axis=1)\n",
    "\n",
    "    # Iterate over each column that needs interpolation\n",
    "    for column in columns_to_interpolate:\n",
    "        if column in df.columns:\n",
    "            # Use 'mask' to isolate parts of the column that should be interpolated\n",
    "            # This will replace values where interpolate_flag is 0 with NaN, which are then not interpolated\n",
    "            mask = df['interpolate_flag'] == 1\n",
    "            # Temporarily store the original data\n",
    "            original_data = df[column].copy()\n",
    "            # Replace data not to be interpolated with NaN\n",
    "            df.loc[~mask, column] = np.nan\n",
    "            # Interpolate missing (NaN) values only where the mask is True\n",
    "            df[column] = df[column].interpolate(method='linear', limit_direction='both')\n",
    "            # Replace the NaN values back with the original data to avoid affecting non-interpolated parts\n",
    "            df.loc[~mask, column] = original_data[~mask]\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_categories(category_dataframe):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Fit the encoder\n",
    "    encoder.fit(category_dataframe)\n",
    "\n",
    "    # Transform the data\n",
    "    encoded_data = encoder.transform(category_dataframe)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "def process_category(category_data):\n",
    "    # Print unique values in the category data\n",
    "    category_keys = np.unique(category_data)\n",
    "\n",
    "    category_df = pd.DataFrame(category_data)\n",
    "\n",
    "    data_array = encode_categories(category_df)\n",
    "\n",
    "    feature_len = np.shape(data_array)[-1]\n",
    "    reshaped_data = data_array.reshape(-1, feature_len)\n",
    "\n",
    "    category_values = np.unique(reshaped_data, axis=0)\n",
    "    \n",
    "    # Convert from np.int32 to int\n",
    "    if type(category_keys[0]) == np.int32:\n",
    "        category_dict = {int(category_keys[i]): category_values[i].tolist() for i in range(len(category_keys))}\n",
    "        \n",
    "    else:\n",
    "        # Compile a dict\n",
    "        category_dict = {category_keys[i]: category_values[i].tolist() for i in range(len(category_keys))}\n",
    "\n",
    "    return reshaped_data, category_dict\n",
    "\n",
    "def sequence_categorical_data(category_data):\n",
    "    for i in range((int(len(category_data) / 24))):\n",
    "        start_idx = i * 24\n",
    "        end_idx = start_idx + 24\n",
    "        block = category_data[start_idx:end_idx]\n",
    "        \n",
    "        # Find the first non-empty string in the block\n",
    "        first_non_empty_string = next((x[0] for x in block if x[0] != ''), None)\n",
    "        \n",
    "        if first_non_empty_string:\n",
    "            # Replace all values in the block with the first non-empty string\n",
    "            category_data[start_idx:end_idx] = first_non_empty_string\n",
    "\n",
    "    # Remove 23 values from every 24 values this just saves memory\n",
    "    category_data = category_data[::24]\n",
    "\n",
    "    return category_data\n",
    "\n",
    "# Takes an np array of dates (y, m, d) and returns two arrays of months and days (x, 1)\n",
    "# TODO update to also return start hour of each sequence!!!\n",
    "def process_date_data(date_data):\n",
    "    # Convert list to df with date col\n",
    "    full_date_df = pd.DataFrame(date_data, columns=['Date'])\n",
    "    full_date_df['Date'] = pd.to_datetime(full_date_df['Date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "    full_date_df['Month'] = full_date_df['Date'].dt.strftime(\"%b\")\n",
    "    full_date_df['Day'] = full_date_df['Date'].dt.day\n",
    "\n",
    "    # Extract the Month and Day columns as NumPy arrays\n",
    "    month_array = full_date_df['Month'].to_numpy().reshape(-1, 1)\n",
    "    day_array = full_date_df['Day'].to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    return month_array, day_array\n",
    "\n",
    "def cleanse_df_blocks(df, block_size = 24):\n",
    "    num_rows = len(df)\n",
    "    num_full_blocks = num_rows // block_size\n",
    "    new_num_rows = num_full_blocks * block_size\n",
    "\n",
    "    # Slice the DataFrame to keep only the rows up to 'new_num_rows'\n",
    "    df = df.iloc[:new_num_rows]\n",
    "\n",
    "    # Step 2: Reset the index to ensure it starts from 0 and is sequential\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Assign block numbers to each row\n",
    "    df['block'] = df.index // block_size\n",
    "\n",
    "    # Step 4: Remove blocks that contain any NaN values\n",
    "    # This function returns True if the block has no NaN values, so it will be kept\n",
    "    df_cleaned = df.groupby('block').filter(lambda x: not x.isnull().values.any())\n",
    "\n",
    "    # Step 5: Drop the 'block' column if it's no longer needed\n",
    "    df_cleaned = df_cleaned.drop(columns=['block'])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, x_columns, y_columns, time_steps=24):\n",
    "    x_sequence = []\n",
    "    y_sequence = []\n",
    "    date_sequence = []\n",
    "    used_indices = []  # List to hold indices used in sequences\n",
    "\n",
    "    # Iterate through the DataFrame to form sequences\n",
    "    for i in range(len(df) - time_steps + 1):\n",
    "\n",
    "        temp_df = df.iloc[i:i + time_steps]\n",
    "\n",
    "        # Check if all entries in the sequence have interpolate_flag set to TRUE and timestamps are consecutive\n",
    "        if temp_df['interpolate_flag'].all() and \\\n",
    "           (temp_df['date'].iloc[-1] - temp_df['date'].iloc[0]).total_seconds() == (time_steps - 1) * 3600:\n",
    "            # Append the sequences to the lists as NumPy arrays\n",
    "            x_sequence.append(temp_df[x_columns].to_numpy())\n",
    "            y_sequence.append(temp_df[y_columns].to_numpy())\n",
    "            date_sequence.append(temp_df['date'].to_numpy())\n",
    "            used_indices.extend(range(i, i + time_steps))  # Add all indices in this sequence to the list\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    x_sequence = np.array(x_sequence)\n",
    "    y_sequence = np.array(y_sequence)\n",
    "    date_sequence = np.array(date_sequence)\n",
    "    # Find indices that are not part of any sequence\n",
    "    unused_indices = set(range(len(df))) - set(used_indices)\n",
    "    unused_rows = df.iloc[list(unused_indices)].sort_index()\n",
    "\n",
    "    return x_sequence, y_sequence, date_sequence, unused_rows\n",
    "\n",
    "\n",
    "def split_data(X, y, location, month, day, test_size=0.2, validation_size=0.15, random_state=42):\n",
    "    # Ensure both arrays have the same length\n",
    "    assert len(X) == len(y), \"The length of X and y must be the same.\"\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create an array of indices and shuffle them\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Shuffle the data arrays using the shuffled indices\n",
    "    X = np.array(X)[indices]\n",
    "    y = np.array(y)[indices]\n",
    "    location = np.array(location)[indices]\n",
    "    month = np.array(month)[indices]\n",
    "    day = np.array(day)[indices]\n",
    "\n",
    "    # Calculate the split index for test data\n",
    "    test_split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    # Split the arrays into train and validation and test sets\n",
    "    X_train_val, X_test = X[:test_split_index], X[test_split_index:]\n",
    "    y_train_val, y_test = y[:test_split_index], y[test_split_index:]\n",
    "    location_train_val, location_test = location[:test_split_index], location[test_split_index:]\n",
    "    month_train_val, month_test = month[:test_split_index], month[test_split_index:]\n",
    "    day_train_val, day_test = day[:test_split_index], day[test_split_index:]\n",
    "\n",
    "    # Calculate the split index for validation data within the training set\n",
    "    validation_split_index = int(len(X_train_val) * (1 - validation_size))\n",
    "\n",
    "    # Split the train and validation arrays into train and validation sets\n",
    "    X_train, X_validate = X_train_val[:validation_split_index], X_train_val[validation_split_index:]\n",
    "    y_train, y_validate = y_train_val[:validation_split_index], y_train_val[validation_split_index:]\n",
    "    location_train, location_validate = location_train_val[:validation_split_index], location_train_val[validation_split_index:]\n",
    "    month_train, month_validate = month_train_val[:validation_split_index], month_train_val[validation_split_index:]\n",
    "    day_train, day_validate = day_train_val[:validation_split_index], day_train_val[validation_split_index:]\n",
    "\n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test, \\\n",
    "        location_train, location_validate, location_test, \\\n",
    "        month_train, month_validate, month_test, \\\n",
    "        day_train, day_validate, day_test\n",
    "\n",
    "def scale_data(train, test, validate):\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit on the training data\n",
    "    scaler.fit(np.concatenate(train))\n",
    "\n",
    "    # Scale the training, test, and validation data\n",
    "    train_scaled = [scaler.transform(df) for df in train]\n",
    "    test_scaled = [scaler.transform(df) for df in test]\n",
    "    validate_scaled = [scaler.transform(df) for df in validate]\n",
    "\n",
    "    return train_scaled, test_scaled, validate_scaled\n",
    "\n",
    "def save_data(directory, data, names):\n",
    "    types = ['train', 'test', 'validate']\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for name in names:\n",
    "        for data_type in types:\n",
    "            file_path = os.path.join(directory, f'{name}_{data_type}.npy')\n",
    "\n",
    "            np.save(file_path, data[i])\n",
    "\n",
    "            i += 1\n",
    "    \n",
    "def initialize_processing(file, COLUMNS_TO_KEEP):    \n",
    "    sensor_readings_df = pd.read_csv(file) # Load in file.\n",
    "\n",
    "    # Step 0: Drop unnecessary columns and columns with a date not falling on the hour\n",
    "    sensor_readings_df = select_useful_columns(sensor_readings_df, COLUMNS_TO_KEEP)\n",
    "    sensor_readings_df = sensor_readings_df[sensor_readings_df['date'].str.endswith(':00:00')]\n",
    "\n",
    "    # Step 0.1: Ensure location remains the same\n",
    "    mean_location = get_mean_location(sensor_readings_df)\n",
    "    sensor_readings_df['latitude'] = mean_location[0]\n",
    "    sensor_readings_df['longitude'] = mean_location[1]\n",
    "\n",
    "    # Step 1: Add is_original column and set to 1\n",
    "    sensor_readings_df['is_original'] = 1\n",
    " \n",
    "    # Step 2: Convert date column to date type\n",
    "    sensor_readings_df['date'] = pd.to_datetime(sensor_readings_df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    sensor_readings_df = sensor_readings_df.sort_values(by='date').reset_index(drop=True)\n",
    " \n",
    "    # Step 3: Create a complete date range\n",
    "    full_range = pd.date_range(start=sensor_readings_df['date'].min(), end=sensor_readings_df['date'].max(), freq='H')\n",
    "    full_df = pd.DataFrame(full_range, columns=['date'])\n",
    " \n",
    "    # Step 4: Merge with the original dataframe\n",
    "    merge_df = pd.merge(full_df, sensor_readings_df, on='date', how='left')\n",
    "    merge_df['is_original'].fillna(0, inplace=True)  # Set is_original to 0 for new rows\n",
    " \n",
    "    # Step 5: Fill other columns for new rows with NaN\n",
    "    for col in set(merge_df.columns) - {'date', 'is_original'}:\n",
    "        if col not in sensor_readings_df:\n",
    "            continue\n",
    "        merge_df[col] = merge_df[col].where(merge_df['is_original'] == 1)\n",
    "    \n",
    "    # Step 7: Mark large gaps in columns - latitude, longitude, sea_water_temperature, and date\n",
    "    # Step 7.1: latitude\n",
    "    lat_mark_df = mark_large_gaps(merge_df, 'latitude', gap_threshold=5)\n",
    "\n",
    "    # Step 7.2: longitude\n",
    "    long_mark_df = mark_large_gaps(lat_mark_df, 'longitude', gap_threshold=5)\n",
    "\n",
    "    # Step 7.3: Sample measurement\n",
    "    sea_temp_mark_df = mark_large_gaps(long_mark_df, 'sea_water_temperature', gap_threshold=5)\n",
    "\n",
    "    # Step 7.4: Date\n",
    "    date_mark_df = mark_large_gaps(sea_temp_mark_df, 'date', gap_threshold=5)\n",
    "\n",
    "    # Step 8: Interpolate small gaps in the columns using linear interpolation\n",
    "    interpolate_df = interpolate_missing_values(date_mark_df, COLUMNS_TO_KEEP)\n",
    "\n",
    "    # Step 9: remove all 24 row blocks with NaN values\n",
    "    # IK not the best but were on a time crunch\n",
    "    # Hello tech debt!\n",
    "    interpolate_df = cleanse_df_blocks(interpolate_df)\n",
    "\n",
    "    # Step 10: add a column for future temperature data\n",
    "    interpolate_df['future_temp'] = interpolate_df['sea_water_temperature'].shift(-24)\n",
    "    interpolate_df.drop(interpolate_df.tail(24).index, inplace=True)\n",
    "    \n",
    "    return interpolate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da79436195b6429c84e140ac93ad7b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gathering and sequencing data:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "RAW_DATA_FOLDER = f'{TRAINING_DATA_FOLDER}/raw/'\n",
    "MAIN_FOLDER = os.listdir(RAW_DATA_FOLDER) # Get all raw data files\n",
    "\n",
    "COVARIATE_COLUMNS = ['latitude', 'longitude', 'date', 'sea_water_temperature']\n",
    "TARGET_COLUMN = 'future_temp'\n",
    "\n",
    "time_steps = 24\n",
    "feature_list = []\n",
    "target_list = []\n",
    "date_list = []\n",
    "\n",
    "file_count = 1#len(MAIN_FOLDER)\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through the files and add to df\n",
    "for i in tqdm(range(file_count), desc='Gathering and sequencing data', unit='files'):\n",
    "    file = f'{RAW_DATA_FOLDER}{MAIN_FOLDER[i]}'\n",
    "\n",
    "    if file.endswith('.csv'):\n",
    "        df = initialize_processing(file, COVARIATE_COLUMNS)\n",
    "\n",
    "        x_sequence, y_sequence, date_sequence, unused_rows = create_sequences(df, COVARIATE_COLUMNS, TARGET_COLUMN, time_steps)\n",
    "\n",
    "        # Append to overall array\n",
    "        feature_list.extend(x_sequence)\n",
    "        target_list.extend(y_sequence)\n",
    "        date_list.extend(date_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and encoding categories\n"
     ]
    }
   ],
   "source": [
    "print('Extracting and encoding categories')\n",
    "# Process all categorical data\n",
    "# Date first\n",
    "date_data = np.array(date_list).reshape(-1, 1)\n",
    "date_data = sequence_categorical_data(date_data)\n",
    "\n",
    "month_data, day_data = process_date_data(date_data)\n",
    "\n",
    "# Location now\n",
    "# Slice and reshape the categorical data (location_code) for encoding\n",
    "latitude_data = np.array(feature_list)[:, :, 0].reshape(-1, 1)\n",
    "longitude_data = np.array(feature_list)[:, :, 1].reshape(-1, 1)\n",
    "\n",
    "latitude_data = sequence_categorical_data(latitude_data)\n",
    "longitude_data = sequence_categorical_data(longitude_data)\n",
    "\n",
    "# Encode all categories\n",
    "encoded_latitude_data, latitude_dict = process_category(latitude_data)\n",
    "encoded_longitude_data, longitude_dict = process_category(longitude_data)\n",
    "encoded_month_data, month_dict = process_category(month_data)\n",
    "encoded_day_data, day_dict = process_category(day_data)\n",
    "\n",
    "file_names = {'latitude_data': latitude_data, 'longitude_data': longitude_data, 'month_data': month_dict, 'day_data': day_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving month_data dictionary\n",
      "Saving day_data dictionary\n",
      "(33862, 24, 2)\n",
      "(33862, 24)\n",
      "(33862, 12)\n",
      "(33862, 31)\n"
     ]
    }
   ],
   "source": [
    "# Save the category dictionaries as json\n",
    "for key, value in file_names.items():\n",
    "    print(f'Saving {key} dictionary')\n",
    "    with open(f\"{TRAINING_DATA_FOLDER}/processed/{key}.json\", 'w') as f:\n",
    "        json.dump(value, f, indent=4)\n",
    "\n",
    "# Print the shapes\n",
    "print(np.shape(feature_list))\n",
    "print(np.shape(target_list))\n",
    "print(np.shape(encoded_location_data))\n",
    "print(np.shape(encoded_month_data))\n",
    "print(np.shape(encoded_day_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33862"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Data processing complete')\n",
    "print('Splitting data')\n",
    "# Remove the 5th column (county) from the feature list\n",
    "feature_list = np.delete(feature_list, 3, axis=2)\n",
    "# Split the data\n",
    "X_train, X_test, X_validate, \\\n",
    "    y_train, y_test, y_validate, \\\n",
    "    location_train, location_test, location_validate, \\\n",
    "    month_train, month_test, month_validate, \\\n",
    "    day_train, day_test, day_validate = \\\n",
    "    split_data(feature_list, target_list, encoded_location_data, encoded_day_data, encoded_month_data)\n",
    "    \n",
    "print('Scaling data')\n",
    "# Scale the data\n",
    "X_train_scaled, X_test_scaled, X_validate_scaled = scale_data(X_train, X_test, X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "print('Saving data')\n",
    "\n",
    "final_data = [X_train_scaled, X_test_scaled, X_validate_scaled,\n",
    "        y_train, y_test, y_validate,\n",
    "        location_train, location_test, location_validate,\n",
    "        month_train, month_test, month_validate,\n",
    "        day_train, day_test, day_validate]\n",
    "\n",
    "data_names = ['X', 'y', 'location', 'month', 'day']\n",
    "\n",
    "# Save the data\n",
    "save_data(f\"{TRAIN_DATA_REGION}/processed/{'filtered' if UNPROCESSED_VERSION == 'Filtered' else 'non-filtered'}\", final_data, data_names)\n",
    "np.save(os.path.join(f\"{TRAIN_DATA_REGION}/processed/{'filtered' if UNPROCESSED_VERSION == 'Filtered' else 'non-filtered'}\", 'X_train_unscaled.npy'), X_train) # To reverse scaling for predictions\n",
    "print('Data processing complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmu_geoai_webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
