{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Designed by Tayven Stover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "import json\n",
    "\n",
    "TRAINING_DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_useful_columns(df, columns_to_keep):\n",
    "    columns = [col for col in df.columns if col not in columns_to_keep]\n",
    "\n",
    "    df = df.drop(columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_mean_location(df):\n",
    "    mean_location = df[['latitude', 'longitude']].mean()\n",
    "\n",
    "    # Convert to list\n",
    "    mean_location = mean_location.to_list()\n",
    "\n",
    "    return mean_location\n",
    "\n",
    "def run_length_encoding(df, column_name):\n",
    "    # Apply RLE: Get run lengths and values\n",
    "    n = len(df)\n",
    "    y = np.array(df[f'{column_name}_is_nan'])\n",
    "    starts = np.r_[0, np.flatnonzero(y[1:] != y[:-1]) + 1]\n",
    "    lengths = np.diff(np.r_[starts, n])\n",
    "    values = y[starts]\n",
    "    return starts, lengths, values\n",
    "\n",
    "def mark_large_gaps(df, column_name, gap_threshold=5):\n",
    "    # Ensure the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"The column '{column_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "     # Flag rows that are NaN\n",
    "    df[f'{column_name}_is_nan'] = df[column_name].isna().astype(int)\n",
    "    \n",
    "    # Apply RLE on the column\n",
    "    starts, lengths, values = run_length_encoding(df, column_name)\n",
    "    \n",
    "    # Initialize the validity column with ones\n",
    "    df[f'{column_name}_valid_sequence'] = 1\n",
    "    \n",
    "    # Identify the start positions of large NaN gaps and their lengths\n",
    "    large_gaps = (values == 1) & (lengths > gap_threshold)\n",
    "\n",
    "    # Set the validity of sequences following large gaps to 0\n",
    "    for start, length in zip(starts[large_gaps], lengths[large_gaps]):\n",
    "        df.loc[start:start+length-1, f'{column_name}_valid_sequence'] = 0\n",
    "    \n",
    "    df.drop([f'{column_name}_is_nan'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def interpolate_missing_values(df, columns_to_interpolate):\n",
    "    # Specify validation columns\n",
    "    validation_columns = [f'{column}_valid_sequence' for column in columns_to_interpolate]\n",
    "    \n",
    "    # Determine rows eligible for interpolation\n",
    "    # Only interpolate rows where all associated validation columns are 1\n",
    "    df['interpolate_flag'] = df[validation_columns].all(axis=1)\n",
    "\n",
    "    # Iterate over each column that needs interpolation\n",
    "    for column in columns_to_interpolate:\n",
    "        if column in df.columns:\n",
    "            # Use 'mask' to isolate parts of the column that should be interpolated\n",
    "            # This will replace values where interpolate_flag is 0 with NaN, which are then not interpolated\n",
    "            mask = df['interpolate_flag'] == 1\n",
    "            # Temporarily store the original data\n",
    "            original_data = df[column].copy()\n",
    "            # Replace data not to be interpolated with NaN\n",
    "            df.loc[~mask, column] = np.nan\n",
    "            # Interpolate missing (NaN) values only where the mask is True\n",
    "            df[column] = df[column].interpolate(method='linear', limit_direction='both')\n",
    "            # Replace the NaN values back with the original data to avoid affecting non-interpolated parts\n",
    "            df.loc[~mask, column] = original_data[~mask]\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_categories(category_dataframe):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Fit the encoder\n",
    "    encoder.fit(category_dataframe)\n",
    "\n",
    "    # Transform the data\n",
    "    encoded_data = encoder.transform(category_dataframe)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "def process_category(category_data):\n",
    "    # Print unique values in the category data\n",
    "    category_keys = np.unique(category_data)\n",
    "\n",
    "    category_df = pd.DataFrame(category_data)\n",
    "\n",
    "    data_array = encode_categories(category_df)\n",
    "\n",
    "    feature_len = np.shape(data_array)[-1]\n",
    "    reshaped_data = data_array.reshape(-1, feature_len)\n",
    "\n",
    "    category_values = np.unique(reshaped_data, axis=0)\n",
    "    \n",
    "    # Convert from np.int32 to int\n",
    "    if type(category_keys[0]) == np.int32:\n",
    "        category_dict = {int(category_keys[i]): category_values[i].tolist() for i in range(len(category_keys))}\n",
    "        \n",
    "    else:\n",
    "        # Compile a dict\n",
    "        category_dict = {category_keys[i]: category_values[i].tolist() for i in range(len(category_keys))}\n",
    "\n",
    "    return reshaped_data, category_dict\n",
    "\n",
    "def sequence_categorical_data(category_data):\n",
    "    for i in range((int(len(category_data) / 24))):\n",
    "        start_idx = i * 24\n",
    "        end_idx = start_idx + 24\n",
    "        block = category_data[start_idx:end_idx]\n",
    "        \n",
    "        # Find the first non-empty string in the block\n",
    "        first_non_empty_string = next((x[0] for x in block if x[0] != ''), None)\n",
    "        \n",
    "        if first_non_empty_string:\n",
    "            # Replace all values in the block with the first non-empty string\n",
    "            category_data[start_idx:end_idx] = first_non_empty_string\n",
    "\n",
    "    # Remove 23 values from every 24 values this just saves memory\n",
    "    category_data = category_data[::24]\n",
    "\n",
    "    return category_data\n",
    "\n",
    "# Takes an np array of dates (y, m, d) and returns two arrays of months and days (x, 1)\n",
    "# TODO update to also return start hour of each sequence!!!\n",
    "def process_date_data(date_data):\n",
    "    # Convert list to df with date col\n",
    "    full_date_df = pd.DataFrame(date_data, columns=['Date'])\n",
    "    full_date_df['Date'] = pd.to_datetime(full_date_df['Date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "    full_date_df['Month'] = full_date_df['Date'].dt.strftime(\"%b\")\n",
    "    full_date_df['Day'] = full_date_df['Date'].dt.day\n",
    "    full_date_df['Hour'] = full_date_df['Date'].dt.hour\n",
    "\n",
    "    # Extract the Month and Day columns as NumPy arrays\n",
    "    month_array = full_date_df['Month'].to_numpy().reshape(-1, 1)\n",
    "    day_array = full_date_df['Day'].to_numpy().reshape(-1, 1)\n",
    "    hour_array = full_date_df['Hour'].to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    return month_array, day_array, hour_array\n",
    "\n",
    "def cleanse_df_blocks(df, block_size = 24):\n",
    "    num_rows = len(df)\n",
    "    num_full_blocks = num_rows // block_size\n",
    "    new_num_rows = num_full_blocks * block_size\n",
    "\n",
    "    # Slice the DataFrame to keep only the rows up to 'new_num_rows'\n",
    "    df = df.iloc[:new_num_rows]\n",
    "\n",
    "    # Step 2: Reset the index to ensure it starts from 0 and is sequential\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Step 3: Assign block numbers to each row\n",
    "    df['block'] = df.index // block_size\n",
    "\n",
    "    # Step 4: Remove blocks that contain any NaN values\n",
    "    # This function returns True if the block has no NaN values, so it will be kept\n",
    "    df_cleaned = df.groupby('block').filter(lambda x: not x.isnull().values.any())\n",
    "\n",
    "    # Step 5: Drop the 'block' column if it's no longer needed\n",
    "    df_cleaned = df_cleaned.drop(columns=['block'])\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, x_columns, y_columns, time_steps=24):\n",
    "    x_sequence = []\n",
    "    y_sequence = []\n",
    "    date_sequence = []\n",
    "    used_indices = []  # List to hold indices used in sequences\n",
    "\n",
    "    # Iterate through the DataFrame to form sequences\n",
    "    for i in range(0, len(df) - time_steps + 1, time_steps):\n",
    "\n",
    "        temp_df = df.iloc[i:i + time_steps]\n",
    "\n",
    "        # Check if all entries in the sequence have interpolate_flag set to TRUE and timestamps are consecutive\n",
    "        if temp_df['interpolate_flag'].all() and \\\n",
    "           (temp_df['date'].iloc[-1] - temp_df['date'].iloc[0]).total_seconds() == (time_steps - 1) * 3600:\n",
    "            # Append the sequences to the lists as NumPy arrays\n",
    "            x_sequence.append(temp_df[x_columns].to_numpy())\n",
    "            y_sequence.append(temp_df[y_columns].to_numpy())\n",
    "            date_sequence.append(temp_df['date'].to_numpy())\n",
    "            used_indices.extend(range(i, i + time_steps))  # Add all indices in this sequence to the list\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    x_sequence = np.array(x_sequence)\n",
    "    y_sequence = np.array(y_sequence)\n",
    "    date_sequence = np.array(date_sequence)\n",
    "    # Find indices that are not part of any sequence\n",
    "    unused_indices = set(range(len(df))) - set(used_indices)\n",
    "    unused_rows = df.iloc[list(unused_indices)].sort_index()\n",
    "\n",
    "    return x_sequence, y_sequence, date_sequence, unused_rows\n",
    "\n",
    "\n",
    "def shuffle_data(*arrays, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(len(arrays[0]))\n",
    "    np.random.shuffle(indices)\n",
    "    return [np.array(arr)[indices] for arr in arrays]\n",
    "\n",
    "def split_by_proportion(arrays, split_ratio):\n",
    "    split_index = int(len(arrays[0]) * split_ratio)\n",
    "    return [arr[:split_index] for arr in arrays], [arr[split_index:] for arr in arrays]\n",
    "\n",
    "def split_data(X, y, latitude, longitude, month, day, hour, test_size=0.2, validation_size=0.15, random_state=42):\n",
    "    # Shuffle all data arrays consistently\n",
    "    X, y, latitude, longitude, month, day, hour = shuffle_data(X, y, latitude, longitude, month, day, hour, random_state=random_state)\n",
    "    \n",
    "    # Split for test set\n",
    "    train_val_data, test_data = split_by_proportion([X, y, latitude, longitude, month, day, hour], 1 - test_size)\n",
    "    X_train_val, y_train_val, lat_train_val, lon_train_val, month_train_val, day_train_val, hour_train_val = train_val_data\n",
    "    X_test, y_test, lat_test, lon_test, month_test, day_test, hour_test = test_data\n",
    "\n",
    "    # Split train and validation sets\n",
    "    train_data, val_data = split_by_proportion([X_train_val, y_train_val, lat_train_val, lon_train_val, month_train_val, day_train_val, hour_train_val], 1 - validation_size)\n",
    "    X_train, y_train, lat_train, lon_train, month_train, day_train, hour_train = train_data\n",
    "    X_validate, y_validate, lat_validate, lon_validate, month_validate, day_validate, hour_validate = val_data\n",
    "\n",
    "    return {\n",
    "        \"train\": (X_train, y_train, lat_train, lon_train, month_train, day_train, hour_train),\n",
    "        \"validate\": (X_validate, y_validate, lat_validate, lon_validate, month_validate, day_validate, hour_validate),\n",
    "        \"test\": (X_test, y_test, lat_test, lon_test, month_test, day_test, hour_test),\n",
    "    }\n",
    "\n",
    "def scale_data(x_train, x_test, x_validate, y_train, y_test, y_validate):\n",
    "    # Initialize separate scalers for X and Y\n",
    "    x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Fit X scaler on the training data\n",
    "    x_scaler.fit(np.concatenate(x_train))\n",
    "\n",
    "    # Scale X data\n",
    "    x_train_scaled = [x_scaler.transform(df) for df in x_train]\n",
    "    x_test_scaled = [x_scaler.transform(df) for df in x_test]\n",
    "    x_validate_scaled = [x_scaler.transform(df) for df in x_validate]\n",
    "\n",
    "    # Reshape Y data for scaling (flattening sample and time dimensions)\n",
    "    y_train_flat = np.array(y_train).reshape(-1, 1)\n",
    "    y_test_flat = np.array(y_test).reshape(-1, 1)\n",
    "    y_validate_flat = np.array(y_validate).reshape(-1, 1)\n",
    "\n",
    "    # Fit Y scaler on the flattened training labels\n",
    "    y_scaler.fit(y_train_flat)\n",
    "\n",
    "    # Scale Y data and reshape back to (n_samp, 24, 1)\n",
    "    y_train_scaled = y_scaler.transform(y_train_flat).reshape(-1, 24, 1)\n",
    "    y_test_scaled = y_scaler.transform(y_test_flat).reshape(-1, 24, 1)\n",
    "    y_validate_scaled = y_scaler.transform(y_validate_flat).reshape(-1, 24, 1)\n",
    "\n",
    "    return x_train_scaled, x_test_scaled, x_validate_scaled, y_train_scaled, y_test_scaled, y_validate_scaled\n",
    "\n",
    "\n",
    "\n",
    "def save_data(directory, data, names):\n",
    "    types = ['train', 'test', 'validate']\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for name in names:\n",
    "        for data_type in types:\n",
    "            file_path = os.path.join(directory, f'{name}_{data_type}.npy')\n",
    "\n",
    "            np.save(file_path, data[i])\n",
    "\n",
    "            i += 1\n",
    "    \n",
    "def initialize_processing(file, COLUMNS_TO_KEEP):    \n",
    "    sensor_readings_df = pd.read_csv(file) # Load in file.\n",
    "\n",
    "    # Step 0: Drop unnecessary columns and columns with a date not falling on the hour\n",
    "    sensor_readings_df = select_useful_columns(sensor_readings_df, COLUMNS_TO_KEEP)\n",
    "    sensor_readings_df = sensor_readings_df[sensor_readings_df['date'].str.endswith(':00:00')]\n",
    "\n",
    "    # Step 0.1: Ensure location remains the same\n",
    "    mean_location = get_mean_location(sensor_readings_df)\n",
    "    sensor_readings_df['latitude'] = mean_location[0]\n",
    "    sensor_readings_df['longitude'] = mean_location[1]\n",
    "\n",
    "    # Step 0.2: Remove all impossible temperature values (less than -2) and (greater than 50)\n",
    "    sensor_readings_df = sensor_readings_df[sensor_readings_df['sea_water_temperature'] >= 0]\n",
    "    sensor_readings_df = sensor_readings_df[sensor_readings_df['sea_water_temperature'] <= 50]\n",
    "\n",
    "    # Step 1: Add is_original column and set to 1\n",
    "    sensor_readings_df['is_original'] = 1\n",
    " \n",
    "    # Step 2: Convert date column to date type\n",
    "    sensor_readings_df['date'] = pd.to_datetime(sensor_readings_df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    sensor_readings_df = sensor_readings_df.sort_values(by='date').reset_index(drop=True)\n",
    " \n",
    "    # Step 3: Create a complete date range\n",
    "    full_range = pd.date_range(start=sensor_readings_df['date'].min(), end=sensor_readings_df['date'].max(), freq='h')\n",
    "    full_df = pd.DataFrame(full_range, columns=['date'])\n",
    " \n",
    "    # Step 4: Merge with the original dataframe\n",
    "    merge_df = pd.merge(full_df, sensor_readings_df, on='date', how='left')\n",
    "    merge_df['is_original'] = merge_df['is_original'].fillna(0) # Set is_original to 0 for new rows\n",
    " \n",
    "    # Step 5: Fill other columns for new rows with NaN\n",
    "    for col in set(merge_df.columns) - {'date', 'is_original'}:\n",
    "        if col not in sensor_readings_df:\n",
    "            continue\n",
    "        merge_df[col] = merge_df[col].where(merge_df['is_original'] == 1)\n",
    "    \n",
    "    # Step 7: Mark large gaps in columns - latitude, longitude, sea_water_temperature, and date\n",
    "    # Step 7.1: latitude\n",
    "    lat_mark_df = mark_large_gaps(merge_df, 'latitude', gap_threshold=5)\n",
    "\n",
    "    # Step 7.2: longitude\n",
    "    long_mark_df = mark_large_gaps(lat_mark_df, 'longitude', gap_threshold=5)\n",
    "\n",
    "    # Step 7.3: Sample measurement\n",
    "    sea_temp_mark_df = mark_large_gaps(long_mark_df, 'sea_water_temperature', gap_threshold=5)\n",
    "\n",
    "    # Step 7.4: Date\n",
    "    date_mark_df = mark_large_gaps(sea_temp_mark_df, 'date', gap_threshold=5)\n",
    "\n",
    "    # Step 8: Interpolate small gaps in the columns using linear interpolation\n",
    "    interpolate_df = interpolate_missing_values(date_mark_df, COLUMNS_TO_KEEP)\n",
    "\n",
    "    # Step 9: remove all 24 row blocks with NaN values\n",
    "    # IK not the best but were on a time crunch\n",
    "    # Hello tech debt!\n",
    "    interpolate_df = cleanse_df_blocks(interpolate_df)\n",
    "\n",
    "    # Step 10: add a column for future temperature data\n",
    "    interpolate_df['future_temp'] = interpolate_df['sea_water_temperature'].shift(-24)\n",
    "    interpolate_df.drop(interpolate_df.tail(24).index, inplace=True)\n",
    "    \n",
    "    return interpolate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35926846ba22492b946bcbfaa319d1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gathering and sequencing data:   0%|          | 0/53 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "RAW_DATA_FOLDER = f'{TRAINING_DATA_FOLDER}/raw/'\n",
    "MAIN_FOLDER = os.listdir(RAW_DATA_FOLDER) # Get all raw data files\n",
    "\n",
    "COVARIATE_COLUMNS = ['latitude', 'longitude', 'date', 'sea_water_temperature']\n",
    "TARGET_COLUMN = 'future_temp'\n",
    "\n",
    "time_steps = 24\n",
    "feature_list = []\n",
    "target_list = []\n",
    "date_list = []\n",
    "\n",
    "file_count = len(MAIN_FOLDER)\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through the files and add to df\n",
    "for i in tqdm(range(file_count), desc='Gathering and sequencing data', unit='files'):\n",
    "    file = f'{RAW_DATA_FOLDER}{MAIN_FOLDER[i]}'\n",
    "\n",
    "    if file.endswith('.csv'):\n",
    "        df = initialize_processing(file, COVARIATE_COLUMNS)\n",
    "\n",
    "        x_sequence, y_sequence, date_sequence, unused_rows = create_sequences(df, COVARIATE_COLUMNS, TARGET_COLUMN, time_steps)\n",
    "\n",
    "        # Append to overall array\n",
    "        feature_list.extend(x_sequence)\n",
    "        target_list.extend(y_sequence)\n",
    "        date_list.extend(date_sequence)\n",
    "\n",
    "# Save feature, target, and date data as .npy incase memory runs out\n",
    "np.save(f'{TRAINING_DATA_FOLDER}/feature_data.npy', feature_list)\n",
    "np.save(f'{TRAINING_DATA_FOLDER}/target_data.npy', target_list)\n",
    "np.save(f'{TRAINING_DATA_FOLDER}/date_data.npy', date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and encoding categories\n"
     ]
    }
   ],
   "source": [
    "print('Extracting and encoding categories')\n",
    "# Process all categorical data\n",
    "# Date first\n",
    "date_data = np.array(date_list).reshape(-1, 1)\n",
    "date_data = sequence_categorical_data(date_data)\n",
    "\n",
    "month_data, day_data, hour_data = process_date_data(date_data)\n",
    "\n",
    "# Location now\n",
    "# Slice and reshape the categorical data (location_code) for encoding\n",
    "latitude_data = np.array(feature_list)[:, :, 0].reshape(-1, 1)\n",
    "longitude_data = np.array(feature_list)[:, :, 1].reshape(-1, 1)\n",
    "\n",
    "latitude_data = sequence_categorical_data(latitude_data)\n",
    "longitude_data = sequence_categorical_data(longitude_data)\n",
    "\n",
    "# Encode all categories\n",
    "encoded_latitude_data, latitude_dict = process_category(latitude_data)\n",
    "encoded_longitude_data, longitude_dict = process_category(longitude_data)\n",
    "encoded_month_data, month_dict = process_category(month_data)\n",
    "encoded_day_data, day_dict = process_category(day_data)\n",
    "encoded_hour_data, hour_dict = process_category(hour_data)\n",
    "\n",
    "file_names = {'latitude_data': latitude_dict, 'longitude_data': longitude_dict, 'month_data': month_dict, 'day_data': day_dict, 'hour_data': hour_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving latitude_data dictionary\n",
      "Saving longitude_data dictionary\n",
      "Saving month_data dictionary\n",
      "Saving day_data dictionary\n",
      "Saving hour_data dictionary\n",
      "(77858, 24, 4)\n",
      "(77858, 24)\n",
      "(77858, 52)\n",
      "(77858, 52)\n",
      "(77858, 12)\n",
      "(77858, 31)\n",
      "(77858, 9)\n"
     ]
    }
   ],
   "source": [
    "# Save the category dictionaries as json\n",
    "for key, value in file_names.items():\n",
    "    print(f'Saving {key} dictionary')\n",
    "    with open(f\"{TRAINING_DATA_FOLDER}/processed/{key}.json\", 'w') as f:\n",
    "        json.dump(value, f, indent=4)\n",
    "\n",
    "# Print the shapes\n",
    "print(np.shape(feature_list))\n",
    "print(np.shape(target_list))\n",
    "print(np.shape(encoded_latitude_data))\n",
    "print(np.shape(encoded_longitude_data))\n",
    "print(np.shape(encoded_month_data))\n",
    "print(np.shape(encoded_day_data))\n",
    "print(np.shape(encoded_hour_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete\n",
      "Splitting data\n",
      "Scaling data\n"
     ]
    }
   ],
   "source": [
    "print('Data processing complete')\n",
    "print('Splitting data')\n",
    "# Remove the first, second and fourth columns from the feature list\n",
    "feature_list = np.delete(feature_list, 0, axis=2)\n",
    "feature_list = np.delete(feature_list, 0, axis=2)\n",
    "feature_list = np.delete(feature_list, 0, axis=2)\n",
    "\n",
    "# Call the function and unpack the returned dictionary\n",
    "splits = split_data(feature_list, target_list, encoded_latitude_data, encoded_longitude_data, encoded_month_data, encoded_day_data, encoded_hour_data, test_size=0.2, validation_size=0.15, random_state=42)\n",
    "\n",
    "# Unpack training, validation, and test sets from the dictionary\n",
    "X_train, y_train, lat_train, lon_train, month_train, day_train, hour_train = splits[\"train\"]\n",
    "X_validate, y_validate, lat_validate, lon_validate, month_validate, day_validate, hour_validate = splits[\"validate\"]\n",
    "X_test, y_test, lat_test, lon_test, month_test, day_test, hour_test = splits[\"test\"]\n",
    "    \n",
    "print('Scaling data')\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled, X_test_scaled, X_validate_scaled, y_train_scaled, y_test_scaled, y_validate_scaled = scale_data(X_train, X_test, X_validate, y_train, y_test, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "Data processing complete\n"
     ]
    }
   ],
   "source": [
    "# Save the data\n",
    "print('Saving data')\n",
    "\n",
    "final_data = [X_train_scaled, X_test_scaled, X_validate_scaled,\n",
    "        y_train_scaled, y_test_scaled, y_validate_scaled,\n",
    "        lat_train, lat_test, lat_validate,\n",
    "        lon_train, lon_test, lon_validate,\n",
    "        month_train, month_test, month_validate,\n",
    "        day_train, day_test, day_validate,\n",
    "        hour_train, hour_test, hour_validate]\n",
    "\n",
    "data_names = ['X', 'y', 'lat', 'lon', 'month', 'day', 'hour']\n",
    "\n",
    "# Save the data\n",
    "save_data(f\"{TRAINING_DATA_FOLDER}/processed/\", final_data, data_names)\n",
    "np.save(os.path.join(f\"{TRAINING_DATA_FOLDER}/processed/\", 'X_train_unscaled.npy'), X_train) # To reverse scaling for predictions\n",
    "np.save(os.path.join(f\"{TRAINING_DATA_FOLDER}/processed/\", 'y_train_unscaled.npy'), y_train) # To reverse scaling for predictions\n",
    "print('Data processing complete')\n",
    "\n",
    "# Delete feature, target, and date data\n",
    "os.remove(f'{TRAINING_DATA_FOLDER}/feature_data.npy')\n",
    "os.remove(f'{TRAINING_DATA_FOLDER}/target_data.npy')\n",
    "os.remove(f'{TRAINING_DATA_FOLDER}/date_data.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai_webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
